# For checking the rag model from IPYNB format

## Implementation Details

### Tools and Libraries
- tesseract-ocr for extracting Bangla texts from the PDF
- Ollama for running open source versions of models locally
- langchain & langchain_community for chunk creation and other functionalities
- FAISS and faiss-cpu for serverless vector database and internal distance function calculation
- sentence-transformer and Huggingface for using various embedding models. Among those used, Huggingface "BAAI/bge-m3" was the most efficient
- Finally, Flask for creating api endpoint and creating a web app that allows users to use the RAG model through a user interface

### Text Extraction
- Used tesseract-ocr for extracting Bangla texts from the given PDF using the OCR method
- Compiled all the text in a text file 
- Used the text file as the primary knowledge base source

### Issues or challenges while formatting and extracting the text
- For being a Bangla text-based PDF, the file could not be extracted through different PDF loader libraries. Thus, the use of OCR technologies was employed.
- Again, the PDF contained various images and tables, which caused further problems while extracting the texts. Much of the context was lost in this process. Any plain-text PDF would have been better for extraction.


### Text Processing
- Uses regex patterns specifically designed for Bangla text cleaning
- Handles special cases for Bangla compound characters
- Preserves Bangla punctuation (ред, ?, !)

### Chunking Approach
- RecursiveCharacterTextSplitter configured with:
  - Chunk size: 200 characters
  - Overlap: 30 characters
  - Separators: ["\n\n", "\n", "ред", "?", "!", ",", " "]
  - Character limit with a 10-20% overlap strategy was used. It is used in many published rag models for best context extraction, as sentences sometimes fail to give full context, and paragraphs most of the time extract too much context. Here, the length-based chunking with some overlap gives out the best context in almost all chunks.

### Embeddings and Retrieval
- Uses HuggingFace's "BAAI/bge-m3" embedding model
- In many Bangla Rag-model-based research papers, this embedding model outperformed others in terms of extracting semantic meanings. Thus it was chosen.
- FAISS for vector storage and similarity search
- Cosine similarity with a threshold of 0.5

### Results and Relevance
- To prevent giving out vague answers, prompt manipulation was used. It was made clear in the prompt that the model will not provide any vague knowledge without any context
- In case of missing context, the model will specify that "He is not capable of answering this question without given context"


### Current Limitations
1. No formal evaluation metrics
2. Basic chunking without semantic awareness
3. No query expansion capabilities
4. The model fails to extract the relevant information chunks 
5. Even if it manages to extract the relevant chunks, it fails to generate a proper response


### Future Improvement
- As stated before, due to limitations in text extraction, much of the knowledge base was left useless. Improving the knowledge base context will help improve the model performance very much.
- A bigger and larger knowledge base will significantly improve the model, as the knowledge base is in Bangla, and not enough context was given out for the model to extract from it.




## How to Use
Install all the dependencies for the project
```bash
pip install -r requirements.txt
```

1. Run cells sequentially in the notebook
2. For queries:
   ```python
   query = "Your Bangla query here"
   response = generate_response(query, vector_db, memory)
   print(response)

   ```

# For accessing the web-app
- Go to the rag-app directory using the command line
```bash
  cd rag-app
```

Install the additional dependencies for the web application
```bash
pip install -r requirements.txt
```

- run the app.py
```bash
python app.py
```
